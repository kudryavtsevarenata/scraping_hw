{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишите скрипт, который будет показывать найдена ли указанная в скрипте веб-страница на сервере.\n",
    "Рекомендуемые библиотеки:\n",
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError\n",
    "from urllib.error import URLError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError\n",
    "from urllib.error import URLError\n",
    "\n",
    "try:\n",
    "    html = urlopen(\"http://not_excisting.com/\")\n",
    "except HTTPError as e:\n",
    "    print(\"HTTP error\")\n",
    "except URLError as e:\n",
    "    print(\"URL error\")\n",
    "else:\n",
    "    print(html.read())\n",
    "\n",
    "\n",
    "try:\n",
    "    html = urlopen(\"http://example.com/\")\n",
    "except HTTPError as e:\n",
    "    print(\"HTTP error\")\n",
    "except URLError as e:\n",
    "    print(\"URL error\")\n",
    "else:\n",
    "    print(\"HTML code\")\n",
    "    print(html.read())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверьте есть ли у сайта SSL сертификат с использованием модуля requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [404]>\n",
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "print(requests.get(\"http://wayback.com/\"))\n",
    "print(requests.get(\"https://do.pstu.ru/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведите при помощи модуля requests блоки \n",
    "Status Code, Headers, Url, History, Encoding, Reason, Cookies, Elapsed, Request и Content с сайта https://python.org.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "resp = requests.get(\"https://python.org\")\n",
    "\n",
    "print(\"Status Code: \" , resp.status_code)\n",
    "print(\"Headers: \" , resp.headers)\n",
    "print(\"Url: \" , resp.url)\n",
    "print(\"History: \" , resp.history)\n",
    "print(\"Encoding: \" , resp.encoding)\n",
    "print(\"Reason: \" , resp.reason)\n",
    "print(\"Cookies: \" , resp.cookies)\n",
    "print(\"Elapsed: \" , resp.elapsed)\n",
    "print(\"Request: \" , resp.request)\n",
    "print(\"Content : \" , resp.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выгрузите и выведете содержимое файла robots.txt с сайта https://en.wikipedia.org/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "resp = requests.get(\"https://en.wikipedia.org/\")\n",
    "\n",
    "tt = resp.text\n",
    "print(\"robots.txt for https://en.wikipedia.org/\")\n",
    "print(tt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Извлеките заголовок h1 из сайта http://www.example.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>Example Domain</h1>\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = urlopen(\"https://example.com/\")\n",
    "bsh = BeautifulSoup(html.read(), 'html.parser')\n",
    "h1 = bsh.find('h1')\n",
    "print(h1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Извлеките и выведете на экран все header теги со страницы https://en.wikipedia.org/wiki/Main_Page с помощью urlopen и BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = urlopen(\"https://en.wikipedia.org/wiki/Main_Page\")\n",
    "soup = BeautifulSoup(html.read(), 'html.parser')\n",
    "\n",
    "res = [str(tag) for tag in soup.find_all('header')]\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выгрузите страницу https://en.wikipedia.org/wiki/Python и сформируйте список ссылок, которые есть на этой странице.\n",
    "Например:\n",
    "#mw-head\n",
    "#p-search\n",
    "https://en.wiktionary.org/wiki/Python\n",
    "https://en.wiktionary.org/wiki/python\n",
    "#Snakes\n",
    "#Ancient_Greece\n",
    "#Media_and_entertainment\n",
    "#Computing\n",
    "#Engineering\n",
    "#Roller_coasters\n",
    "#Vehicles\n",
    "#Weaponry\n",
    "#See_also\n",
    "/w/index.php?title=Python&action=edit§ion=1\n",
    "…\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = urlopen(\"https://en.wikipedia.org/wiki/Python\")\n",
    "soup = BeautifulSoup(html.read(), 'html.parser')\n",
    "for elem in soup.find_all('a', href=True):\n",
    "    print(elem['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выгрузите вот эту CSV: http://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/4.5_month.csv и посчитайте кол-во строк в ней.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "594\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = urlopen(\"http://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/4.5_month.csv\")\n",
    "soup = BeautifulSoup(html.read(), 'html.parser')\n",
    "print(len(str(soup).split('\\n')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Суммируя весь предыдущий опыт напишите программу, которая будет скрейпить данные из imdb. Рекомендуемые библиотеки: BeautifulSoup, requests и random. Программа должна выполнять следующий функционал:\n",
    "При запуске программа должна подключаться к странице https://www.imdb.com/chart/top;\n",
    "Собирать список фильмов (и по вашему желанию их описание)\n",
    "Выводить 10 случайных фильмов\n",
    "Пример вывода:\n",
    "\n",
    "--------------------------------------------\n",
    "Lock, Stock and Two Smoking Barrels (1998)\n",
    "A botched card game in London triggers four friends, thugs, weed-growers, hard gangsters, loan sharks and debt collectors to collide with each other in a series of unexpected events, all for the sake of weed, cash and two antique shotguns.\n",
    "\n",
    "--------------------------------------------\n",
    "Wild Strawberries (1957)\n",
    "After living a life marked by coldness, an aging professor is forced to confront the emptiness of his existence.\n",
    "\n",
    "--------------------------------------------\n",
    "M (1931)\n",
    "When the police in a German city are unable to catch a child-murderer, other criminals join in the manhunt.\n",
    "\n",
    "--------------------------------------------\n",
    "The Seventh Seal (1957)\n",
    "A man seeks answers about life, death, and the existence of God as he plays chess against the Grim Reaper during the Black Plague.\n",
    "\n",
    "--------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185. Бен-Гур\n",
      "After a Jewish prince is betrayed and sent into slavery by a Roman friend in 1st-century Jerusalem, he regains his freedom and comes back for revenge.\n",
      "----------------------\n",
      "114. Афера\n",
      "Two grifters team up to pull off the ultimate con.\n",
      "----------------------\n",
      "72. Доктор Стрейнджлав, или Как я научился не волноваться и полюбил атомную бомбу\n",
      "An insane American general orders a bombing attack on the Soviet Union, triggering a path to nuclear holocaust that a war room full of politicians and generals frantically tries to stop.\n",
      "----------------------\n",
      "139. Казино\n",
      "In Las Vegas, two best friends - a casino executive and a mafia enforcer - compete for a gambling empire and a fast-living, fast-loving socialite.\n",
      "----------------------\n",
      "185. Бен-Гур\n",
      "After a Jewish prince is betrayed and sent into slavery by a Roman friend in 1st-century Jerusalem, he regains his freedom and comes back for revenge.\n",
      "----------------------\n",
      "152. Нечто\n",
      "A research team in Antarctica is hunted by a shape-shifting alien that assumes the appearance of its victims.\n",
      "----------------------\n",
      "199. Общество мёртвых поэтов\n",
      "Maverick teacher John Keating returns in 1959 to the prestigious New England boys' boarding school where he was once a star student, using poetry to embolden his pupils to new heights of self-expression.\n",
      "----------------------\n",
      "236. La battaglia di Algeri\n",
      "In the 1950s, fear and violence escalate as the people of Algiers fight for independence from the French government.\n",
      "----------------------\n",
      "242. Pather Panchali\n",
      "Impoverished priest Harihar Ray, dreaming of a better life for himself and his family, leaves his rural Bengal village in search of work.\n",
      "----------------------\n",
      "149. Старикам тут не место\n",
      "Violence and mayhem ensue after a hunter stumbles upon a drug deal gone wrong and more than two million dollars in cash near the Rio Grande.\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "\n",
    "def get_soup_from_url(url, headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.5845.931 YaBrowser/23.9.3.931 Yowser/2.5 Safari/537.36'}):\n",
    "    r = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "def make_title_and_href(href):\n",
    "    d = dict()\n",
    "    for item in href:\n",
    "        if item.string:\n",
    "            list_from_href = item['href'].split('/')\n",
    "            d[item.string] = \"https://www.imdb.com/\" + list_from_href[1] + \"/\" + list_from_href[2] + \"/plotsummary/\" + list_from_href[3]\n",
    "    return d\n",
    "\n",
    "def make_list_title(href):\n",
    "    l = list()\n",
    "    for item in href:\n",
    "        if item.string is not None:\n",
    "            l.append(item.string)\n",
    "    return l\n",
    "\n",
    "\n",
    "soup = get_soup_from_url(\"https://www.imdb.com/chart/top/\")\n",
    "movies_href = soup.find_all('a', class_=\"ipc-title-link-wrapper\")\n",
    "\n",
    "\n",
    "d = make_title_and_href(movies_href)\n",
    "l = make_list_title(movies_href)\n",
    "\n",
    "for i in range(10):\n",
    "    elem = l[random.randint(0,len(l) - 1)]\n",
    "    print(elem)\n",
    "    soap = get_soup_from_url(d[elem])\n",
    "    info = soap.find('div', class_='ipc-html-content-inner-div').string\n",
    "    print(info)\n",
    "    print(\"----------------------\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e952f3f3b0740b9aa52f718669564e989426e2044d90ed4082f4796055a10afb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
